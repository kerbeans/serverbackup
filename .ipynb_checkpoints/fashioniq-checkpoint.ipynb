{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca86dad-d2ca-4443-b28b-e45d2a0b7c06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "from diffusers import StableDiffusionImg2ImgPipeline \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f042099-dcec-4746-9fd4-43fc113137e6",
   "metadata": {},
   "source": [
    "## inherit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8357c-9274-489e-9048-742a115e940c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from typing import Callable, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import PIL\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "from diffusers.configuration_utils import FrozenDict\n",
    "from diffusers.models import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers.pipeline_utils import DiffusionPipeline\n",
    "from diffusers.schedulers import (\n",
    "    DDIMScheduler,\n",
    "    EulerAncestralDiscreteScheduler,\n",
    "    EulerDiscreteScheduler,\n",
    "    LMSDiscreteScheduler,\n",
    "    PNDMScheduler,\n",
    ")\n",
    "from diffusers.utils import deprecate, logging\n",
    "from diffusers import StableDiffusionPipelineOutput\n",
    "from diffusers.safety_checker import StableDiffusionSafetyChecker\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n",
    "\n",
    "\n",
    "def preprocess(image):\n",
    "    w, h = image.size\n",
    "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
    "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.0 * image - 1.0\n",
    "\n",
    "\n",
    "class FashionPipeline(StableDiffusionImg2ImgPipeline):\n",
    "    r\"\"\"\n",
    "    Pipeline for text-guided image to image generation using Stable Diffusion.\n",
    "\n",
    "    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n",
    "    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n",
    "\n",
    "    Args:\n",
    "        vae ([`AutoencoderKL`]):\n",
    "            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n",
    "        text_encoder ([`CLIPTextModel`]):\n",
    "            Frozen text-encoder. Stable Diffusion uses the text portion of\n",
    "            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n",
    "            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n",
    "        tokenizer (`CLIPTokenizer`):\n",
    "            Tokenizer of class\n",
    "            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n",
    "        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n",
    "        scheduler ([`SchedulerMixin`]):\n",
    "            A scheduler to be used in combination with `unet` to denoise the encoded image latens. Can be one of\n",
    "            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n",
    "        safety_checker ([`StableDiffusionSafetyChecker`]):\n",
    "            Classification module that estimates whether generated images could be considered offensive or harmful.\n",
    "            Please, refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5) for details.\n",
    "        feature_extractor ([`CLIPFeatureExtractor`]):\n",
    "            Model that extracts features from generated images to be used as inputs for the `safety_checker`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vae: AutoencoderKL,\n",
    "        text_encoder: CLIPTextModel,\n",
    "        tokenizer: CLIPTokenizer,\n",
    "        unet: UNet2DConditionModel,\n",
    "        scheduler: Union[\n",
    "            DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler, EulerDiscreteScheduler, EulerAncestralDiscreteScheduler\n",
    "        ],\n",
    "        safety_checker: StableDiffusionSafetyChecker,\n",
    "        feature_extractor: CLIPFeatureExtractor,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n",
    "            deprecation_message = (\n",
    "                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n",
    "                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n",
    "                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n",
    "                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n",
    "                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n",
    "                \" file\"\n",
    "            )\n",
    "            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
    "            new_config = dict(scheduler.config)\n",
    "            new_config[\"steps_offset\"] = 1\n",
    "            scheduler._internal_dict = FrozenDict(new_config)\n",
    "\n",
    "        if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is True:\n",
    "            deprecation_message = (\n",
    "                f\"The configuration file of this scheduler: {scheduler} has not set the configuration `clip_sample`.\"\n",
    "                \" `clip_sample` should be set to False in the configuration file. Please make sure to update the\"\n",
    "                \" config accordingly as not setting `clip_sample` in the config might lead to incorrect results in\"\n",
    "                \" future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very\"\n",
    "                \" nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\"\n",
    "            )\n",
    "            deprecate(\"clip_sample not set\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
    "            new_config = dict(scheduler.config)\n",
    "            new_config[\"clip_sample\"] = False\n",
    "            scheduler._internal_dict = FrozenDict(new_config)\n",
    "\n",
    "        if safety_checker is None:\n",
    "            logger.warn(\n",
    "                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n",
    "                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n",
    "                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n",
    "                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n",
    "                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n",
    "                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n",
    "            )\n",
    "\n",
    "        self.register_modules(\n",
    "            vae=vae,\n",
    "            text_encoder=text_encoder,\n",
    "            tokenizer=tokenizer,\n",
    "            unet=unet,\n",
    "            scheduler=scheduler,\n",
    "            safety_checker=safety_checker,\n",
    "            feature_extractor=feature_extractor,\n",
    "        )\n",
    "\n",
    "    def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n",
    "        r\"\"\"\n",
    "        Enable sliced attention computation.\n",
    "\n",
    "        When this option is enabled, the attention module will split the input tensor in slices, to compute attention\n",
    "        in several steps. This is useful to save some memory in exchange for a small speed decrease.\n",
    "\n",
    "        Args:\n",
    "            slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n",
    "                When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n",
    "                a number is provided, uses as many slices as `attention_head_dim // slice_size`. In this case,\n",
    "                `attention_head_dim` must be a multiple of `slice_size`.\n",
    "        \"\"\"\n",
    "        if slice_size == \"auto\":\n",
    "            # half the attention head size is usually a good trade-off between\n",
    "            # speed and memory\n",
    "            slice_size = self.unet.config.attention_head_dim // 2\n",
    "        self.unet.set_attention_slice(slice_size)\n",
    "\n",
    "    def disable_attention_slicing(self):\n",
    "        r\"\"\"\n",
    "        Disable sliced attention computation. If `enable_attention_slicing` was previously invoked, this method will go\n",
    "        back to computing attention in one step.\n",
    "        \"\"\"\n",
    "        # set slice_size = `None` to disable `set_attention_slice`\n",
    "        self.enable_attention_slicing(None)\n",
    "\n",
    "    def enable_xformers_memory_efficient_attention(self):\n",
    "        r\"\"\"\n",
    "        Enable memory efficient attention as implemented in xformers.\n",
    "\n",
    "        When this option is enabled, you should observe lower GPU memory usage and a potential speed up at inference\n",
    "        time. Speed up at training time is not guaranteed.\n",
    "\n",
    "        Warning: When Memory Efficient Attention and Sliced attention are both enabled, the Memory Efficient Attention\n",
    "        is used.\n",
    "        \"\"\"\n",
    "        self.unet.set_use_memory_efficient_attention_xformers(True)\n",
    "\n",
    "    def disable_xformers_memory_efficient_attention(self):\n",
    "        r\"\"\"\n",
    "        Disable memory efficient attention as implemented in xformers.\n",
    "        \"\"\"\n",
    "        self.unet.set_use_memory_efficient_attention_xformers(False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]],\n",
    "        init_image: Union[torch.FloatTensor, PIL.Image.Image],\n",
    "        strength: float = 0.8,\n",
    "        num_inference_steps: Optional[int] = 50,\n",
    "        guidance_scale: Optional[float] = 7.5,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        num_images_per_prompt: Optional[int] = 1,\n",
    "        eta: Optional[float] = 0.0,\n",
    "        generator: Optional[torch.Generator] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
    "        callback_steps: Optional[int] = 1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Function invoked when calling the pipeline for generation.\n",
    "\n",
    "        Args:\n",
    "            prompt (`str` or `List[str]`):\n",
    "                The prompt or prompts to guide the image generation.\n",
    "            init_image (`torch.FloatTensor` or `PIL.Image.Image`):\n",
    "                `Image`, or tensor representing an image batch, that will be used as the starting point for the\n",
    "                process.\n",
    "            strength (`float`, *optional*, defaults to 0.8):\n",
    "                Conceptually, indicates how much to transform the reference `init_image`. Must be between 0 and 1.\n",
    "                `init_image` will be used as a starting point, adding more noise to it the larger the `strength`. The\n",
    "                number of denoising steps depends on the amount of noise initially added. When `strength` is 1, added\n",
    "                noise will be maximum and the denoising process will run for the full number of iterations specified in\n",
    "                `num_inference_steps`. A value of 1, therefore, essentially ignores `init_image`.\n",
    "            num_inference_steps (`int`, *optional*, defaults to 50):\n",
    "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
    "                expense of slower inference. This parameter will be modulated by `strength`.\n",
    "            guidance_scale (`float`, *optional*, defaults to 7.5):\n",
    "                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n",
    "                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n",
    "                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n",
    "                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n",
    "                usually at the expense of lower image quality.\n",
    "            negative_prompt (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n",
    "                if `guidance_scale` is less than `1`).\n",
    "            num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
    "                The number of images to generate per prompt.\n",
    "            eta (`float`, *optional*, defaults to 0.0):\n",
    "                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n",
    "                [`schedulers.DDIMScheduler`], will be ignored for others.\n",
    "            generator (`torch.Generator`, *optional*):\n",
    "                A [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation\n",
    "                deterministic.\n",
    "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
    "                The output format of the generate image. Choose between\n",
    "                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n",
    "                plain tuple.\n",
    "            callback (`Callable`, *optional*):\n",
    "                A function that will be called every `callback_steps` steps during inference. The function will be\n",
    "                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n",
    "            callback_steps (`int`, *optional*, defaults to 1):\n",
    "                The frequency at which the `callback` function will be called. If not specified, the callback will be\n",
    "                called at every step.\n",
    "\n",
    "        Returns:\n",
    "            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n",
    "            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n",
    "            When returning a tuple, the first element is a list with the generated images, and the second element is a\n",
    "            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n",
    "            (nsfw) content, according to the `safety_checker`.\n",
    "        \"\"\"\n",
    "        if isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
    "\n",
    "        if strength < 0 or strength > 1:\n",
    "            raise ValueError(f\"The value of strength should in [0.0, 1.0] but is {strength}\")\n",
    "\n",
    "        if (callback_steps is None) or (\n",
    "            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n",
    "                f\" {type(callback_steps)}.\"\n",
    "            )\n",
    "\n",
    "        # set timesteps\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        if isinstance(init_image, PIL.Image.Image):\n",
    "            init_image = preprocess(init_image)\n",
    "\n",
    "        # get prompt text embeddings\n",
    "        text_inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_input_ids = text_inputs.input_ids\n",
    "\n",
    "        if text_input_ids.shape[-1] > self.tokenizer.model_max_length:\n",
    "            removed_text = self.tokenizer.batch_decode(text_input_ids[:, self.tokenizer.model_max_length :])\n",
    "            logger.warning(\n",
    "                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n",
    "                f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n",
    "            )\n",
    "            text_input_ids = text_input_ids[:, : self.tokenizer.model_max_length]\n",
    "        text_embeddings = self.text_encoder(text_input_ids.to(self.device))[0]\n",
    "\n",
    "        # duplicate text embeddings for each generation per prompt\n",
    "        text_embeddings = text_embeddings.repeat_interleave(num_images_per_prompt, dim=0)\n",
    "\n",
    "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
    "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
    "        # corresponds to doing no classifier free guidance.\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "        # get unconditional embeddings for classifier free guidance\n",
    "        if do_classifier_free_guidance:\n",
    "            uncond_tokens: List[str]\n",
    "            if negative_prompt is None:\n",
    "                uncond_tokens = [\"\"] * batch_size\n",
    "            elif type(prompt) is not type(negative_prompt):\n",
    "                raise TypeError(\n",
    "                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n",
    "                    f\" {type(prompt)}.\"\n",
    "                )\n",
    "            elif isinstance(negative_prompt, str):\n",
    "                uncond_tokens = [negative_prompt]\n",
    "            elif batch_size != len(negative_prompt):\n",
    "                raise ValueError(\"The length of `negative_prompt` should be equal to batch_size.\")\n",
    "            else:\n",
    "                uncond_tokens = negative_prompt\n",
    "\n",
    "            max_length = text_input_ids.shape[-1]\n",
    "            uncond_input = self.tokenizer(\n",
    "                uncond_tokens,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
    "\n",
    "            # duplicate unconditional embeddings for each generation per prompt\n",
    "            seq_len = uncond_embeddings.shape[1]\n",
    "            uncond_embeddings = uncond_embeddings.repeat(1, num_images_per_prompt, 1)\n",
    "            uncond_embeddings = uncond_embeddings.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "            # For classifier free guidance, we need to do two forward passes.\n",
    "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
    "            # to avoid doing two forward passes\n",
    "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "        # encode the init image into latents and scale the latents\n",
    "        latents_dtype = text_embeddings.dtype\n",
    "        init_image = init_image.to(device=self.device, dtype=latents_dtype)\n",
    "        init_latent_dist = self.vae.encode(init_image).latent_dist\n",
    "        init_latents = init_latent_dist.sample(generator=generator)\n",
    "        init_latents = 0.18215 * init_latents\n",
    "\n",
    "        if isinstance(prompt, str):\n",
    "            prompt = [prompt]\n",
    "        if len(prompt) > init_latents.shape[0] and len(prompt) % init_latents.shape[0] == 0:\n",
    "            # expand init_latents for batch_size\n",
    "            deprecation_message = (\n",
    "                f\"You have passed {len(prompt)} text prompts (`prompt`), but only {init_latents.shape[0]} initial\"\n",
    "                \" images (`init_image`). Initial images are now duplicating to match the number of text prompts. Note\"\n",
    "                \" that this behavior is deprecated and will be removed in a version 1.0.0. Please make sure to update\"\n",
    "                \" your script to pass as many init images as text prompts to suppress this warning.\"\n",
    "            )\n",
    "            deprecate(\"len(prompt) != len(init_image)\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
    "            additional_image_per_prompt = len(prompt) // init_latents.shape[0]\n",
    "            init_latents = torch.cat([init_latents] * additional_image_per_prompt * num_images_per_prompt, dim=0)\n",
    "        elif len(prompt) > init_latents.shape[0] and len(prompt) % init_latents.shape[0] != 0:\n",
    "            raise ValueError(\n",
    "                f\"Cannot duplicate `init_image` of batch size {init_latents.shape[0]} to {len(prompt)} text prompts.\"\n",
    "            )\n",
    "        else:\n",
    "            init_latents = torch.cat([init_latents] * num_images_per_prompt, dim=0)\n",
    "\n",
    "        # get the original timestep using init_timestep\n",
    "        offset = self.scheduler.config.get(\"steps_offset\", 0)\n",
    "        init_timestep = int(num_inference_steps * strength) + offset\n",
    "        init_timestep = min(init_timestep, num_inference_steps)\n",
    "\n",
    "        timesteps = self.scheduler.timesteps[-init_timestep]\n",
    "        timesteps = torch.tensor([timesteps] * batch_size * num_images_per_prompt, device=self.device)\n",
    "\n",
    "        # add noise to latents using the timesteps\n",
    "        noise = torch.randn(init_latents.shape, generator=generator, device=self.device, dtype=latents_dtype)\n",
    "        init_latents = self.scheduler.add_noise(init_latents, noise, timesteps)\n",
    "\n",
    "        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
    "        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
    "        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
    "        # and should be between [0, 1]\n",
    "        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
    "        extra_step_kwargs = {}\n",
    "        if accepts_eta:\n",
    "            extra_step_kwargs[\"eta\"] = eta\n",
    "\n",
    "        # check if the scheduler accepts generator\n",
    "        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
    "        if accepts_generator:\n",
    "            extra_step_kwargs[\"generator\"] = generator\n",
    "\n",
    "        latents = init_latents\n",
    "\n",
    "        t_start = max(num_inference_steps - init_timestep + offset, 0)\n",
    "\n",
    "        # Some schedulers like PNDM have timesteps as arrays\n",
    "        # It's more optimized to move all timesteps to correct device beforehand\n",
    "        timesteps = self.scheduler.timesteps[t_start:].to(self.device)\n",
    "\n",
    "        for i, t in enumerate(self.progress_bar(timesteps)):\n",
    "            # expand the latents if we are doing classifier free guidance\n",
    "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "            # predict the noise residual\n",
    "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "            # perform guidance\n",
    "            if do_classifier_free_guidance:\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
    "\n",
    "            # call the callback, if provided\n",
    "            if callback is not None and i % callback_steps == 0:\n",
    "                callback(i, t, latents)\n",
    "\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        image = self.vae.decode(latents).sample\n",
    "\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "        if self.safety_checker is not None:\n",
    "            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(\n",
    "                self.device\n",
    "            )\n",
    "            image, has_nsfw_concept = self.safety_checker(\n",
    "                images=image, clip_input=safety_checker_input.pixel_values.to(text_embeddings.dtype)\n",
    "            )\n",
    "        else:\n",
    "            has_nsfw_concept = None\n",
    "\n",
    "        if output_type == \"pil\":\n",
    "            image = self.numpy_to_pil(image)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image, has_nsfw_concept)\n",
    "\n",
    "        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b5125c-1620-422a-831a-f2f3a3b647ae",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279f2281-8302-41a3-bf67-6a5656a78d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "data_path='../data'\n",
    "\n",
    "class FashionIQDataset(Dataset):\n",
    "    \"\"\"\n",
    "    FashionIQ dataset class which manage FashionIQ data.\n",
    "    The dataset can be used in 'relative' or 'classic' mode:\n",
    "        - In 'classic' mode the dataset yield tuples made of (image_name, image)\n",
    "        - In 'relative' mode the dataset yield tuples made of:\n",
    "            - (reference_image, target_image, image_captions) when split == train\n",
    "            - (reference_name, target_name, image_captions) when split == val   ??? reference_image, target_image\n",
    "            - (reference_name, reference_image, image_captions) when split == test\n",
    "    The dataset manage an arbitrary numbers of FashionIQ category, e.g. only dress, dress+toptee+shirt, dress+shirt...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split: str, dress_types: List[str], mode: str, preprocess: callable):\n",
    "        \"\"\"\n",
    "        :param split: dataset split, should be in ['test', 'train', 'val']\n",
    "        :param dress_types: list of fashionIQ category\n",
    "        :param mode: dataset mode, should be in ['relative', 'classic']:\n",
    "            - In 'classic' mode the dataset yield tuples made of (image_name, image) ??? temporarily deleted\n",
    "            - In 'relative' mode the dataset yield tuples made of:\n",
    "                - (reference_image, target_image, image_captions) when split == train\n",
    "                - (reference_name, target_name, image_captions) when split == val\n",
    "                - (reference_name, reference_image, image_captions) when split == test\n",
    "        :param preprocess: function which preprocesses the image\n",
    "        \"\"\"\n",
    "        self.mode = mode\n",
    "        self.dress_types = dress_types\n",
    "        self.split = split\n",
    "\n",
    "        if mode not in ['relative', 'classic']:\n",
    "            raise ValueError(\"mode should be in ['relative', 'classic']\")\n",
    "        if split not in ['test', 'train', 'val']:\n",
    "            raise ValueError(\"split should be in ['test', 'train', 'val']\")\n",
    "        for dress_type in dress_types:\n",
    "            if dress_type not in ['dress', 'shirt', 'toptee']:\n",
    "                raise ValueError(\"dress_type should be in ['dress', 'shirt', 'toptee']\")\n",
    "\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "        # get triplets made by (reference_image, target_image, a pair of relative captions)\n",
    "        self.triplets: List[dict] = []\n",
    "        for dress_type in dress_types:\n",
    "            with open(base_path / 'data' / 'captions' / f'cap.{dress_type}.{split}1.json') as f:\n",
    "                self.triplets.extend(json.load(f))\n",
    "        self.triplets=self.triplets# mode\n",
    "        # get the image names\n",
    "        self.image_names: list = []\n",
    "        for dress_type in dress_types:\n",
    "            with open(base_path / 'data' / 'image_splits' / f'split.{dress_type}.{split}1.json') as f:\n",
    "                self.image_names.extend(json.load(f))\n",
    "\n",
    "        print(f\"FashionIQ {split} - {dress_types} dataset in {mode} mode initialized\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            if self.mode == 'relative':\n",
    "                image_captions = self.triplets[index]['captions']\n",
    "                reference_name = self.triplets[index]['candidate']\n",
    "\n",
    "                if self.split == 'train':\n",
    "                    reference_image_path = base_path / 'fashionIQ_dataset' / 'images' / f\"{reference_name}.jpg\"\n",
    "                    reference_image = self.preprocess(PIL.Image.open(reference_image_path))\n",
    "                    target_name = self.triplets[index]['target']\n",
    "                    target_image_path = base_path / 'fashionIQ_dataset' / 'images' / f\"{target_name}.jpg\"\n",
    "                    target_image = self.preprocess(PIL.Image.open(target_image_path))\n",
    "                    return reference_image, target_image, image_captions\n",
    "\n",
    "                elif self.split == 'val':\n",
    "                    target_name = self.triplets[index]['target']\n",
    "                    return reference_name, target_name, image_captions\n",
    "\n",
    "                elif self.split == 'test':\n",
    "                    reference_image_path = base_path / 'data' / 'images' / f\"{reference_name}.jpg\"\n",
    "                    reference_image = self.preprocess(PIL.Image.open(reference_image_path))\n",
    "                    return reference_name, reference_image, image_captions\n",
    "\n",
    "            elif self.mode == 'classic':\n",
    "                image_name = self.image_names[index]\n",
    "                image_path = base_path / 'data' / 'images' / f\"{image_name}.jpg\"\n",
    "                image = self.preprocess(PIL.Image.open(image_path))\n",
    "                return image_name, image\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"mode should be in ['relative', 'classic']\")\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 'relative':\n",
    "            return len(self.triplets)\n",
    "        elif self.mode == 'classic':\n",
    "            return len(self.image_names)\n",
    "        else:\n",
    "            raise ValueError(\"mode should be in ['relative', 'classic']\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2250f1bb-1f8a-4a90-9349-ef776e408d39",
   "metadata": {},
   "source": [
    "## finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "388dd5d0-537a-4c1f-8cd0-ee4a45e98622",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'diffusers.train_util'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainloop\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'diffusers.train_util'"
     ]
    }
   ],
   "source": [
    "from diffusers.train_util import Trainloop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b36bb7-10cb-49fa-a52e-fe0ba560a8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "diffusion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
